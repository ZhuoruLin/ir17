\relax 
\citation{blitzstein2014introduction}
\newlabel{cond_prob_ising}{{3}{1}}
\newlabel{yieq1}{{5}{2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Ising model Gibbs sampling}}{2}}
\citation{Brown}
\citation{Brown}
\citation{Brown}
\citation{Brown}
\citation{Brown}
\citation{Brown}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A tree-structured factor graph in which four factors link four random variables. Variable $x_2$ takes one of three discrete states, and the other three variables are binary. \cite  {Brown}}}{6}}
\newlabel{fig:tree}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A factor graph representation of a LDPC code linking four factor (parity constraint) nodes to eight variable (message bit) nodes. The unary factors encode noisy observations of the message bits from the output of some communications channel. \cite  {Brown}}}{6}}
\newlabel{fig:factor}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Posterior beliefs on each bits after 50 iterations. Note that this does not guarantee to happen every time and is depend on the received codeword quality. Problem 3-c provide a better assessment of the model performance.}}{9}}
\newlabel{fig:p3-ptb}{{3}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Hamming distances vs iteration for 10 different Monte Carlo trail.}}{10}}
\newlabel{fig:p3-ptc-1}{{4}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Hamming distances vs iteration for 10 different Monte Carlo trail at error rate 0.08.}}{11}}
\newlabel{fig:p3-ptd-1}{{5}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Hamming distances vs iteration for 10 different Monte Carlo trail at error rate 0.10.}}{11}}
\newlabel{fig:p3-ptd-2}{{6}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Image decoding at error rate 0.06}}{12}}
\newlabel{fig:p3-pte}{{7}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Repeat part e with error rate 0.10.}}{13}}
\newlabel{fig:p3-ptf}{{8}{13}}
\citation{choi_cvpr10}
\citation{choi_cvpr10}
\citation{choi_cvpr10}
\citation{Chow68approximatingdiscrete}
\citation{choi_cvpr10}
\citation{choi_cvpr10}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Using context within object detection for computer vision. \cite  {choi_cvpr10} }}{14}}
\newlabel{fig:object_rec}{{9}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Pairwise MRF of object class presences in images \cite  {choi_cvpr10}. Red edges denote negative correlations between classes. The thickness of each edge represents the strength of the link. You will be learning this MRF in question 3.}}{15}}
\newlabel{fig:object_rec2}{{10}{15}}
\gdef\minted@oldcachelist{,
  default.pygstyle,
  default-pyg-prefix.pygstyle,
  3AD052B8F55D9F0D69886086CA79C36C0758A3B8FA3C3C429D51E52876FB2014.pygtex,
  53BDBD5656C6170D06078AEE2E877C850758A3B8FA3C3C429D51E52876FB2014.pygtex,
  18C558DBB6C4938E6815AF8B8DE4A56F0758A3B8FA3C3C429D51E52876FB2014.pygtex,
  D73B31E3F3FB8EADA341885661E1D0650758A3B8FA3C3C429D51E52876FB2014.pygtex,
  BD2E7B595E6940ECDA92C7F9EF58A76D0758A3B8FA3C3C429D51E52876FB2014.pygtex,
  CE59CD6208F70A83E13BF26870AE1E480758A3B8FA3C3C429D51E52876FB2014.pygtex,
  127FE17444E6D843B2E6B96FF4175A170758A3B8FA3C3C429D51E52876FB2014.pygtex}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Potential graph visualization. The closer in the graph the higher two nodes have higher probability to occurs together in a picture. You can clearly see some combinations that makes sense. Such as sky-mountain, sea-boat, shoes-bag, and window-door.}}{18}}
